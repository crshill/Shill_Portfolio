{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on MNIST\n",
    "Using Keras on the MNIST data set. We will use a Convolutional Neural Network because CNN's are well suited for image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train our model on a 28x28 image, so we must shape our data as such. But how we import each picture depends on where the color channel is listed. We must acount for this. This is gray scale so there is only one channel dimension as oposed to 3 for RBG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert our train and test labels to be categorical in one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out one of the training images with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a 2D convolution of the image - it's set up to take 32 windows, or \"filters\", of each image, each filter being 3x3 in size. We then run a second convolution on top of that with 64 3x3 windows (recommended within Keras's examples).\n",
    "\n",
    "Next we apply a MaxPooling2D layer.\n",
    "\n",
    "A dropout filter is then applied to prevent overfitting.\n",
    "\n",
    "Next we flatten the 2D layer into a 1D layer, and can now treat this as a traditional multi-layer perceptron...\n",
    "\n",
    "Then feed that into a hidden, flat layer of 128 units.\n",
    "\n",
    "We then apply dropout again to prevent overfitting.\n",
    "\n",
    "And finally, softmax is applied to choose our category of 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using adam as the optimizer, but RMSProp would work very well also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train our model. Limiting the batch size to make things run a little faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 1026s - loss: 0.1926 - acc: 0.9418 - val_loss: 0.0499 - val_acc: 0.9834\n",
      "Epoch 2/10\n",
      " - 995s - loss: 0.0817 - acc: 0.9759 - val_loss: 0.0397 - val_acc: 0.9874\n",
      "Epoch 3/10\n",
      " - 996s - loss: 0.0633 - acc: 0.9811 - val_loss: 0.0339 - val_acc: 0.9895\n",
      "Epoch 4/10\n",
      " - 991s - loss: 0.0518 - acc: 0.9836 - val_loss: 0.0302 - val_acc: 0.9909\n",
      "Epoch 5/10\n",
      " - 996s - loss: 0.0442 - acc: 0.9861 - val_loss: 0.0322 - val_acc: 0.9905\n",
      "Epoch 6/10\n",
      " - 994s - loss: 0.0395 - acc: 0.9878 - val_loss: 0.0303 - val_acc: 0.9898\n",
      "Epoch 7/10\n",
      " - 1001s - loss: 0.0329 - acc: 0.9890 - val_loss: 0.0328 - val_acc: 0.9907\n",
      "Epoch 8/10\n",
      " - 993s - loss: 0.0298 - acc: 0.9907 - val_loss: 0.0336 - val_acc: 0.9916\n",
      "Epoch 9/10\n",
      " - 998s - loss: 0.0296 - acc: 0.9911 - val_loss: 0.0281 - val_acc: 0.9915\n",
      "Epoch 10/10\n",
      " - 996s - loss: 0.0252 - acc: 0.9917 - val_loss: 0.0340 - val_acc: 0.9918\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.034049834153382426\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
